{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Module 1. Introduction to Tensorflow\n",
    "The purpose of this Notebook is to provide an introduction to Tensorflow 2.0 for creating basic machine learning models. To this end, we are going to be working with an example, which consist in classifying flowers attending to differerent input features (e.g., sepal length/width and the petal length/width.) For this we are going to use an opensource dataset known as the [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris). The first part of this module will focus on downloading, and preparing the data. Do not hurry nor worry if you do not get all details yet. There are many ways of doing this task in Tensorflow (as well as in other frameworks), you will get well versed in doing this with experience. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Donwloading and preparing the dataset\n",
    "In this case we are going to download the data using the python request API. Particularly, we will use the method `get` to download the data and will write it into a local file 'iris.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "r = requests.get(url)\n",
    "\n",
    "# Writing data to a file\n",
    "with open('iris.data', 'wb') as f:\n",
    "  f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can validate that you have create the file by listing the content of the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the content of the file you just created in the row above, you will see that there is one line per element in the dataset. Each of these lines is a comma separated list of values, which are either numerical or text. The first four columns of every line are numerical and indicate respectively the value of four different features we have for each flower: tthe sepal length, the sepal witdth, the petal width and the petal length. The last column in the line is the name of the flower type in text. The first 5 lines of the file are as follows:\n",
    "```\n",
    "5.1,3.5,1.4,0.2,Iris-setosa\n",
    "4.9,3.0,1.4,0.2,Iris-setosa\n",
    "4.7,3.2,1.3,0.2,Iris-setosa\n",
    "4.6,3.1,1.5,0.2,Iris-setosa\n",
    "5.0,3.6,1.4,0.2,Iris-setosa\n",
    "\n",
    "```\n",
    "Obviously, this is going to be different for different datasets. There is no receipt for this: each dataset creator decides what is bests or makes more sense. Your task should always be reading the documentation associated to the dataset to understand its structure. \n",
    "\n",
    "\n",
    "Let's move on with our dataset though. We are going to use pandas now to load the dataset into a dataframe so it will be easier for us to manipulate the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv('iris.data',names=['sepal_length', 'sepal_width', 'petal_width', 'petal_length', 'label'], header=None)\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will focus on the numerical values of the dataframe. We will extract them into a separate dataframe and will center these values around the mean of eavery column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_values = iris_df[[\"sepal_length\", \"sepal_width\", \"petal_width\", \"petal_length\"]]\n",
    "numerical_values = numerical_values - numerical_values.mean(axis=0)\n",
    "numerical_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe contains numerical values, but these are not yet prepared to be the input of a machine learning model. In Tensorflow, we need to use the `tf.Tensor` tensor type. Tensors are n-dimensional structures that can hold different type of data. In addition, tensors have additional methods and storage that facilitate computing and storing gradients for example. A tensor might or might not share the underlaying memory of another python structure (such as a dataframe or nd-array), but it definitely adds some additional features on top fo these. \n",
    "\n",
    "A `tf.Tensor` can be created out of a dataframe using the function `tf.convert_to_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = tf.convert_to_tensor(numerical_values)\n",
    "print(input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the features are numerical values and have been converted into tensors, they are ready to be use for training a model. However the labels are still in text form. Recall from the slides, that a machine learning model is a mathematical functions and these require numerical representations. In this example, we are working with labels, or categories for doing a classification. A typical numerical representation that is used for this kind of problems is called `one hot` encoding. Basically, each label is represented by a vector of length equal to the number of different existing categories. In our case, we have three types of flowers, so this vector will have lenght three. The components of the vector will be all `zero`, except one, which will have the value `one`. This non-zero component will indicate the type of the label represented by that vector. For example, we might decide that the `Iris-setosa` type use the first vector element, `Iris-versicolor` type use the second vector element, and `Iris-virginica` use the third vector component. To convert a set of categories into this representation we need to make use of the tensorflow functions tf.one_hot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df['label'] = iris_df['label'].map({'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2})\n",
    "output = tf.one_hot(iris_df['label'],depth=3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model\n",
    "In the next step, we will use a neural network for classifying the input values. In particular, for this example, we will use a fully connected neural network consisting of four layers: \n",
    "- Input layer with as many neurons as input features we have\n",
    "- A hidden layer of 32 neurons\n",
    "- A hidden layer of 16 neurons\n",
    "- An output layer with as many neurons as categories we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "{     \"tags\": [         \"hide-cell\"     ] }"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential([\n",
    "        Dense(32,activation='relu',input_shape=(4,)),\n",
    "        Dense(16,activation='relu'),\n",
    "        Dense(3, activation='softmax')])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the input layer is the input itself and we only need to indicate this as the shape of the first hidden neuron in our model. The second dimension of the `input_shape` parameter in this case is left empty to indicate that elements can be provided as input to the model in batches of unspecified size. This is a cool feature because as we know the training algorithm works in batches. \n",
    "\n",
    "The model will provide a prediction as we know. Would would like to measure the quality of that prediction with a loss function. In this case, as we are working with categories, we will use the `categorical_crossentropy` function, which measures the difference between probability distributions for a given random variable/set of events. We also would need an algorithm for training the network. In this example we will use a vanilla stochastic gradient descent one. Finally, we will be also interested in knowing the value of some metrics during the training phase. In this example, we will be interested to know how many flowers have been classified correctly. To pass all these arguments to our model, we will make use of the compile function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all these parameters are defined, we can train our model. As our features and output are already tensors, we can use them as input directly. Training can be trigered with the `tf.fit` function. This function accept different parameters. Besides the input and outputs of the model, we are going to set the values for the batch size (`batch_size`) and number of epochs (`epochs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(input_features,output,batch_size=64,epochs=25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have now created a model that is able to predict with around 75% accuracy the type of a flower given four features regarding its dimension. But we have left so many things on the table: how do we use the model, how does the model generalise, how do we improve the model, etc. Do not worry, we are going to be covering these in the next modules. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
