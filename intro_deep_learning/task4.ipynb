{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55987154",
   "metadata": {},
   "source": [
    "# Module 4. Model deployment and transfer learning\n",
    "\n",
    "In this notebook we deal with the usual scenario found often in real life: it is time to establish a reasonable end-to-end system for training your model. Depending on the problem you are solving, you need to design the baseline network type and architecture. In this step, ask yourself questions like:\n",
    "\n",
    "- Which kind of convolution filters should I use?\n",
    "- How deep should my network be?\n",
    "- Which activation type should I use?\n",
    "- What kind of optimizer should I use?\n",
    "- Do I need to add any other regularization layers to avoid overfitting?\n",
    "\n",
    "# Overfiting and regularization layers\n",
    "\n",
    "The main cause of poor performance in machine learning (ML) is typically either overfitting or underfitting the training dataset. \n",
    "\n",
    "- Underfitting occurs when the model is too simple to learn the training data, resulting in poor performance on the training data.\n",
    "- Overfitting, on the other hand, happens when the model is overly complex for the problem at hand. Instead of learning features that fit the training data, it memorizes the training data. Consequently, it performs very well on the training data but fails to generalize when tested with new data that it hasn't seen before.\n",
    "\n",
    "## Regularization techniques to avoid overfitting\n",
    "\n",
    "If you observe that your neural network is overfitting the training data, your network might be too complex and need to be simplified. One of the first techniques you should try is regularization. In the previous notebook we already saw one regularization technique: data augmentation. In this notebook we review other regularization techniques. \n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout is another regularization technique. It is effective for simplifying a neural network and thus avoiding overfitting. The algorithm is fairly simple: at every training iteration, every neuron has a probability p of being temporarily ignored (dropped out) during this training iteration. This means it may be active during subsequent iterations. \n",
    "\n",
    "Although it is counterintuitive to intentionally pause the learning on some of the network neurons, it is quite surprising how well this technique works. The probability p is a hyperparameter that is called dropout rate. \n",
    "\n",
    "> Dropout helps reduce interdependent learning among the neurons. In that sense, it helps to view dropout as a form of ensemble learning. In ensemble learning, we train a number of weaker classifiers separately, and then we use them at test time by averaging the responses of all ensemble members. Since each classifier has been trained separately, it has learned different aspects of the data, and their mistakes (errors) are different. Combining them helps to produce a stronger classifier, which is less prone to overfitting.\n",
    "> \n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "When we train a model on a certain dataset, we make certain assumptions about the distribution of the data. However, when we are dealing with real-world data, the distribution is not always constant. This is where the idea of covariate shift comes in. Covariate shift refers to the phenomenon where the distribution of the input data changes between the training phase and the testing phase. In other words, the model is being tested on a different distribution than the one it was trained on. This can lead to poor performance of the model, even if it performed very well during training. Therefore, it is important to take into account the possibility of covariate shift when training machine learning models, and to use techniques addressing this issue.\n",
    "\n",
    "Covariate shift can occur in neural networks as the values of parameters in previous layers change, causing the activation values in later layers to change as well. Batch norm helps reduce the degree of change in the distribution of hidden unit values, providing more stability for later layers of the network.\n",
    "\n",
    "Batch normalization adds an operation in the neural network just before the activation function of each layer to do the following:\n",
    "\n",
    "1. Zero-center the inputs\n",
    "2. Normalize the zero-centered inputs\n",
    "3. Scale and shift the results\n",
    "\n",
    "This operation lets the model learn the optimal scale and mean of the inputs for each layer.\n",
    "\n",
    "Now is the time for you to put in practice these ideas. In the first part of this notebook you will deal with a slightly more complex classification problem: Cifar10. You will be asked to define a model for it. Feel free to use any of the aforementioned regularization techniques for the problem. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "\n",
    "# use tensorflow dataset to load cifa10 dataset\n",
    "cifar10, info = tfds.load('cifar10',data_dir='cifar10_data',download=True,shuffle_files=True,with_info=True, as_supervised=True)\n",
    "\n",
    "# extrat train and test dataset\n",
    "ds_train = cifar10['train']\n",
    "ds_test  = cifar10['test']\n",
    "\n",
    "# show some examples\n",
    "tfds.show_examples(ds_train,info)\n",
    "\n",
    "# shuffle, batch and prefetch train dataset\n",
    "ds_train = ds_train.shuffle(1024).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# batch and prefetch test dataset\n",
    "ds_test = ds_test.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# define a normalise function\n",
    "def normalise_img(image, label):\n",
    "    return tf.cast(image,tf.float32) / 255., tf.one_hot(label,depth=10)\n",
    "\n",
    "# map normalise function to train and test dataset\n",
    "ds_train = ds_train.map(normalise_img,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test  = ds_test.map(normalise_img,num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3bbb22",
   "metadata": {},
   "source": [
    "Create your model in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(ds_train, epochs=15, validation_data=ds_test)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39b024",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "If your problem is similar to another problem that has been studied extensively, you should first copy the model and algorithm that are known to perform the best for that task. You can even use a model that was trained on a different dataset for your own problem without having to train it from scratch. This is called transfer learning.\n",
    "\n",
    "Transfer learning involves taking a pre-trained model and retraining it on a task that has some overlap with the original training task. The analogy of a builder skilled in one material wanting to learn how to use another one is applicable to model deployment and transfer learning, as the skills learned in one area can be valuable in another.\n",
    "\n",
    "As an example in deep learning, consider a pre-trained model that recognizes different types of cars very well. We want to train a model to recognize types of trucks, and many of the insights gained from the car model would be useful, such as the ability to recognize headlights and wheels.\n",
    "\n",
    "Transfer learning is especially powerful when we do not have a large and varied dataset. In this case, a model trained from scratch would likely memorize the training data quickly, but not generalize well to new data. With transfer learning, you can increase your chances of training an accurate and robust model on a small dataset.\n",
    "\n",
    "Despite the Cifar10 is not precisely a small dataset, we are exploring in the following the idea of transfer learning. For this we are going to download a model that has been trained on a different dataset (Imagenet) for classifying images into a set of different classes. We need to adapt that trained model for classifying our dataset onto a completely different set of images. \n",
    "\n",
    "The part of the model that is actually doing the classification are the fully connected layers at the deppest part of the model. This part of the model is therefore not useful for our new task and we can discard it. The convolutional layers at the beginning of the model, are simply in charge of extracting features of the images (and these features might be useful for different tasks - thus beneficial for transfer learning). This part we should take and we can either freeze (i.e., not allowing the training to change them anymore and assuming that the features that these layers extrat are enough for our new tasks) or further train (to adapt them to extract new possible features related to the new task.)\n",
    "\n",
    "An example is showed in the next cell. The `include_top=False` indicates we only want to download the convolutional layers of that trained model on the `imagenet` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.applications.DenseNet201(weights='imagenet', include_top=False, input_shape=(32,32,3))\n",
    "\n",
    "# if you want to further trainthese downloaded layers you can uncomment the following piece of code\n",
    "for layer in dense.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93063b8",
   "metadata": {},
   "source": [
    "The downloaded model alone does nothing. In order to have a model that allows classifying the Cifar-10 dataset we need to add to the models some layers for doing the classification. Perform this task in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05530fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "#model.add(tf.keras.layers.Lambda(lambda image: tf.image.resize(image, (224,224))))\n",
    "model.add(dense)\n",
    "# add a flatten layer\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "history=model.fit(ds_train, epochs=10, validation_data=ds_test)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de2504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to further trainthese downloaded layers you can uncomment the following piece of code\n",
    "for layer in dense.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "history=model.fit(ds_train, epochs=10, validation_data=ds_test)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
